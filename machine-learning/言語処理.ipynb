{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev2\"><a href=\"#用語\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>用語</a></div><div class=\"lev3\"><a href=\"#bug-of-words\"><span class=\"toc-item-num\">0.1.1&nbsp;&nbsp;</span>bug of words</a></div><div class=\"lev3\"><a href=\"#Levenshtein-Distance\"><span class=\"toc-item-num\">0.1.2&nbsp;&nbsp;</span>Levenshtein Distance</a></div><div class=\"lev2\"><a href=\"#fuzzy-word-matching\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>fuzzy word matching</a></div><div class=\"lev3\"><a href=\"#ライブラリ\"><span class=\"toc-item-num\">0.2.1&nbsp;&nbsp;</span>ライブラリ</a></div><div class=\"lev2\"><a href=\"#word-embedding\"><span class=\"toc-item-num\">0.3&nbsp;&nbsp;</span>word embedding</a></div><div class=\"lev3\"><a href=\"#word2vec\"><span class=\"toc-item-num\">0.3.1&nbsp;&nbsp;</span>word2vec</a></div><div class=\"lev3\"><a href=\"#GloVe\"><span class=\"toc-item-num\">0.3.2&nbsp;&nbsp;</span>GloVe</a></div><div class=\"lev3\"><a href=\"#LSI-(Latent-Semantic-Indexing)\"><span class=\"toc-item-num\">0.3.3&nbsp;&nbsp;</span>LSI (Latent Semantic Indexing)</a></div><div class=\"lev2\"><a href=\"#tf-idf\"><span class=\"toc-item-num\">0.4&nbsp;&nbsp;</span>tf-idf</a></div><div class=\"lev2\"><a href=\"#word-net\"><span class=\"toc-item-num\">0.5&nbsp;&nbsp;</span>word net</a></div><div class=\"lev3\"><a href=\"#Jaro-Winkler距離\"><span class=\"toc-item-num\">0.5.1&nbsp;&nbsp;</span>Jaro-Winkler距離</a></div><div class=\"lev4\"><a href=\"#定義\"><span class=\"toc-item-num\">0.5.1.1&nbsp;&nbsp;</span>定義</a></div><div class=\"lev4\"><a href=\"#Jaro-Winkler距離\"><span class=\"toc-item-num\">0.5.1.2&nbsp;&nbsp;</span>Jaro-Winkler距離</a></div><div class=\"lev2\"><a href=\"#LSTM\"><span class=\"toc-item-num\">0.6&nbsp;&nbsp;</span>LSTM</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用語\n",
    "### bug of words\n",
    "単語の出現頻度ベクトル\n",
    "\n",
    "### Levenshtein Distance\n",
    "編集距離のこと\n",
    "\n",
    "## fuzzy word matching\n",
    "次のサブ問題に分けて考えられる\n",
    "\n",
    "1. 与えられた文字列の中からおよその部分文字列を見つける\n",
    "2. およその部分文字列にマッチする辞書内の文字列を探す\n",
    "\n",
    "編集距離をもとに行う\n",
    "\n",
    "### ライブラリ\n",
    "* [fuzzywuzzy](https://github.com/seatgeek/fuzzywuzzy)\n",
    "\n",
    "比較する単語を用意しないとダメ→結局手作業 正規表現とかよりはまだマシ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word embedding\n",
    "単語を多次元のベクトルに埋め込む手法の一般的な呼称\n",
    "\n",
    "### word2vec\n",
    "word embeddingのひとつ。\n",
    "\n",
    "各単語を数百次元のベクトルで表現することで意味とかについての計算ができるようになる。\n",
    "\n",
    "内部のアルゴリズムは2層のニューラルネット\n",
    "\n",
    "predictive model\n",
    "\n",
    "Loss(target word | context word,; Vectors)を小さくするように学習する\n",
    "\n",
    "BilBOWA : word2vecのマルチリンガルバージョン\n",
    "\n",
    "### GloVe\n",
    "共起頻度から単語のベクトルを計算する。\n",
    "\n",
    "共起行列から次元縮約を行って、もとを再現できるような縮約をする\n",
    "\n",
    "トレーニングデータが必要\n",
    "\n",
    "count-based model\n",
    "\n",
    "\n",
    "word2vecとGloVeの違い　 : https://www.quora.com/How-is-GloVe-different-from-word2vec\n",
    "\n",
    "パラメーターをそろえたらあまり変わらないかもね"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LSI (Latent Semantic Indexing) \n",
    "次元を圧縮する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf\n",
    "tf \\* idfの値。あるドキュメント集合において、あるドキュメントの、ある単語につけられる。tf-idfが高い語は重要であると考えることができる。\n",
    "\n",
    "* tf ( Term Frequency) : その単語の、そのドキュメントでの出現回数 / そのドキュメントで出現したすべての単語の総数\n",
    "* idf (Inverse Document Frequency ) : dfの逆数。ただし計算しやすいようにlogで対数をとる。 log(1/df) 。あるドキュメントでしか使われていないような特徴的な単語の値が大きくなる。\n",
    "* df (Document Frequency) : その単語が出現したドキュメントの数 / 全ドキュメントの数。いろいろなドキュメントで使われていたら大きくなる。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word net\n",
    "概念辞書。上位語、下位語、同意語などをとってこれる。\n",
    "\n",
    "ここでの類似度とは、ツリー構造の分類上で共通の上位概念にどれだけ早く辿り着くかなので、意味上の類似度とは一致していない。\n",
    "\n",
    "> 下位語の場合、 人間が名詞の属性を見つけることのできる早さは、その特徴を定義している階層を見つける早さに依存していることが心理学実験で明らかになっている。 したがってカナリアは鳴き鳥の一種である（直下の下位語となっている）ため、人は「カナリアは歌う」かどうかをすぐに判断することができるが、 「カナリアは飛ぶ」かどうかを判断するにはもう少し時間がかかり（二層の隔たりがある）、「カナリアは皮膚を持っている」かどうかを判断するにはより多くの時間を要する（複数の階層の隔たりがある）。これは、人間はある概念と他の似た概念を区別するのに必要なもっとも明確な情報のみを保持していることから、 人間がWordNetに似た方法で意味の情報を記憶しているということを示唆している。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaro-Winkler距離\n",
    "JW距離が高いほうが、一致度が高い\n",
    "\n",
    "数学的な距離の定義は満たしていないし、まったく同じじゃなくても1になる。\n",
    "\n",
    "#### 定義\n",
    "\n",
    "ふたつの文字列$s_{1}$、$s_{2}$に対して、Jaro距離$d_{j}$は\n",
    "\n",
    "\n",
    "\n",
    "$d_{j} = \n",
    " \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      0 \\: {\\rm if} \\, m = 0\\\\\n",
    "      \\frac{1}{3} (\\frac{m}{|s_{1}|}+\\frac{m}{s_{2}}+\\frac{m-t}{m})\\:{\\rm  otherwise}\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* m : マッチした文字の数\n",
    "* t : transpositionsの数/2\n",
    "\n",
    "文字列$s_{1}$、$s_{2}$からそれぞれとってきた文字が*マッチしている*とは、その2つの文字が同じかつ、\n",
    "\n",
    "$\\lfloor\\frac{\\max(|s_{1}|,|s_{2}|)}{2} \\rfloor -1$よりも離れていないときである。\n",
    "\n",
    "$s_{1}$のすべての文字に対して、$s_{2}$のすべての文字との比較が行われる。同じだけど位置が違うマッチした文字数を2で割った数がt\n",
    "\n",
    "例）CRATEとTRACE\n",
    "\n",
    "R,A,Eがマッチ→m=3。\n",
    "\n",
    "C,Tは出てくるけど、距離がfloor(max(5,5)/2)-1=1より遠いからダメ。\n",
    "\n",
    "t=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaro-Winkler距離\n",
    "Jaro-Winkler距離は、先頭から一致している文字列に重みpを与える。\n",
    "\n",
    "$d_{w} = d_{j} + ({\\cal l}p(1-d_{j}))$\n",
    "\n",
    "* $\\cal l$ : 最大4文字の共通するプレフィックス\n",
    "* p: どれくらいプレフィックスに重みをつけるか。0.25以下（$s_{w}\\leq1$）。基本的にp=0.1\n",
    "\n",
    "例） MARTHAとMARHTA\n",
    "\n",
    "* m = 6\n",
    "* len(s_\\{1,2\\}) = 6\n",
    "* t = 2 /2 = 1\n",
    "* $d_{j}= 1/3 (6/6  + 6/6 + 6-1/6) = 0.944$\n",
    "* l = 3\n",
    "* $d_{w} = 0.944 + (3 * 0.1 (1-0.944)) = 0.961$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LSTM\n",
    "LSTM(Long short-term memory)は、RNN(Recurrent Neural Network)の拡張として1995年に登場した、時系列データ(sequential data)に対するモデル、あるいは構造(architecture)の1種です。\n",
    "\n",
    "http://qiita.com/t_Signull/items/21b82be280b46f467d1b\n",
    "\n",
    "\n",
    "長期記憶をもつようにできる\n",
    "\n",
    "誤差逆伝播法\n",
    "\n",
    "RNN\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
