{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev2\"><a href=\"#2.-ユニグラム\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>2. ユニグラム</a></div><div class=\"lev3\"><a href=\"#2.3-最尤推定\"><span class=\"toc-item-num\">0.1.1&nbsp;&nbsp;</span>2.3 最尤推定</a></div><div class=\"lev2\"><a href=\"#2.4-MAP推定\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>2.4 MAP推定</a></div><div class=\"lev3\"><a href=\"#2.5-ベイズ推定\"><span class=\"toc-item-num\">0.2.1&nbsp;&nbsp;</span>2.5 ベイズ推定</a></div><div class=\"lev3\"><a href=\"#2.7-ハイパーパラメータ推定\"><span class=\"toc-item-num\">0.2.2&nbsp;&nbsp;</span>2.7 ハイパーパラメータ推定</a></div><div class=\"lev2\"><a href=\"#3.-混合ユニグラムモデル\"><span class=\"toc-item-num\">0.3&nbsp;&nbsp;</span>3. 混合ユニグラムモデル</a></div><div class=\"lev3\"><a href=\"#3.1-混合ユニグラムモデル\"><span class=\"toc-item-num\">0.3.1&nbsp;&nbsp;</span>3.1 混合ユニグラムモデル</a></div><div class=\"lev3\"><a href=\"#3.3-EMアルゴリズム\"><span class=\"toc-item-num\">0.3.2&nbsp;&nbsp;</span>3.3 EMアルゴリズム</a></div><div class=\"lev3\"><a href=\"#3.4-変分ベイズ推定\"><span class=\"toc-item-num\">0.3.3&nbsp;&nbsp;</span>3.4 変分ベイズ推定</a></div><div class=\"lev4\"><a href=\"#3.4.1-周辺尤度の最大化\"><span class=\"toc-item-num\">0.3.3.1&nbsp;&nbsp;</span>3.4.1 周辺尤度の最大化</a></div><div class=\"lev3\"><a href=\"#3.5-ギブスサンプリング\"><span class=\"toc-item-num\">0.3.4&nbsp;&nbsp;</span>3.5 ギブスサンプリング</a></div><div class=\"lev4\"><a href=\"#3.5.1-MCMC\"><span class=\"toc-item-num\">0.3.4.1&nbsp;&nbsp;</span>3.5.1 MCMC</a></div><div class=\"lev4\"><a href=\"#3.5.2-パラメータの周辺化\"><span class=\"toc-item-num\">0.3.4.2&nbsp;&nbsp;</span>3.5.2 パラメータの周辺化</a></div><div class=\"lev2\"><a href=\"#4.-トピックモデル\"><span class=\"toc-item-num\">0.4&nbsp;&nbsp;</span>4. トピックモデル</a></div><div class=\"lev2\"><a href=\"#8.-トピック数の推定\"><span class=\"toc-item-num\">0.5&nbsp;&nbsp;</span>8. トピック数の推定</a></div><div class=\"lev4\"><a href=\"#8.1.2-中華料理店過程\"><span class=\"toc-item-num\">0.5.0.1&nbsp;&nbsp;</span>8.1.2 中華料理店過程</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ユニグラム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カテゴリ分布のパラメータ$\\phi$が与えられたときの文書集合Wの確率は\n",
    "\n",
    "$p(W|\\phi)=\\Pi^{D}_{d=1}\\Pi^{N_d}_{n=1}p(w_{dn}|\\phi)=\\Pi^{V}_{v=1}\\phi_v^{N_{v}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 最尤推定\n",
    "ある文章Wが与えられたときに、ユニグラムモデルの未知パラメータ$\\phi$を推定したい。最尤推定で求める。\n",
    "\n",
    "$\\rm{argmax}_{\\phi} \\log p(W|\\phi)$ ただし$\\sum_{v=1}^{V}\\phi_v=1$\n",
    "\n",
    "ラグランジュの未定乗数法を用いる。次の関数が極値をとるパラメータの値を求める。\n",
    "\n",
    "$F=\\log p(W|\\phi) + \\lambda (\\sum_{v=1}^{V}\\phi_v - 1 )$\n",
    "\n",
    "$\\frac{\\partial F}{\\partial \\phi_v} = \\frac{N_v}{\\phi_v}+\\lambda=0$\n",
    "\n",
    "$\\frac{\\partial F}{\\partial \\lambda} = \\sum_v \\phi_v - 1= 0$\n",
    "\n",
    "より、$\\phi_v = \\frac{N_v}{N}$\n",
    "\n",
    "つまり、単純に観測した頻度分布をとればいい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 MAP推定\n",
    "例えば、サイコロの出る目の確率を求めるときに1回しか観測しないと、その目しか出ない、ということになる。それはおかしい。過学習をさけるためにMAP推定をする。\n",
    "\n",
    "カテゴリ分布のパラメータの事前分布$p(\\phi|\\beta)$（$\\phi$と$\\beta$を与えると、その$\\phi$になる確率が返ってくる。）は共役事前分布をとると計算が楽。指数型分布族の場合、共役事前分布が存在する。カテゴリ分布の共役事前分布はディリクレ分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ディリクレ分布：\n",
    "\n",
    "$\\rm{DIrichlet}(\\phi|\\beta)= \\frac{\\Gamma (\\sum^V_{v=1}\\beta_v)}{\\Pi^V_{v=1}\\Gamma(\\beta_v)}\\Pi^V_{v=1}\\phi_v^{\\beta_v-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータ$\\beta$は$\\phi$と同じ次元。$\\beta_v>0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータを$(\\beta, \\cdots, \\beta)$とすると、事前分布は\n",
    "\n",
    "$P(\\phi|\\beta)=\\rm{Dirichlet}(\\phi|\\beta,\\cdots,\\beta)=\\frac{\\Gamma(\\beta V)}{\\Gamma(\\beta)^V}\\Pi^{V}_{v=1}\\phi^{\\beta-1}_{v}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事後確率の対数をとる\n",
    "\n",
    "$\\rm{argmax}_{\\phi}[\\log p(\\phi|W,\\beta)] =\\rm{argmax}_{\\phi}[\\log p(W|\\phi) + \\log p(\\phi|\\beta)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$=\\rm{argmax}_{\\phi}[\\sum^V_{v=1}\\log \\phi^{N_v}_v+\\log\\Gamma(\\beta V)-V\\log\\gamma(\\beta) + \\sum^V_{v=1}\\log \\phi^{\\beta-1}_v]$\n",
    "\n",
    "$=\\rm{argmax}_{\\phi}[\\sum^V_{v=1}(N_v+\\beta-1)\\log\\phi_v]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ラグランジュの未定乗数法を使うと、\n",
    "\n",
    "$\\phi_v=\\frac{N_v+\\beta-1}{N+(\\beta-1)V}$\n",
    "\n",
    "となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 ベイズ推定\n",
    "MAP推定では、どれくらい推定値が確からしいのか分からない。観測回数が少ないときと多いときで同じ値になるという問題がある。事後確率自体を求める。\n",
    "\n",
    "$p(\\phi|W,\\beta) = \\frac{p(W|\\phi)p(\\phi|\\beta)}{\\int p(\\phi|\\beta)p(W|\\phi)d\\phi}$\n",
    "\n",
    "$=Dir(\\phi|N_1+\\beta,\\cdots,N_v+\\beta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 ハイパーパラメータ推定\n",
    "初期値として与えるハイパーパラメータβはどう決めたらいいのか？という問題が残る。何かしら事前知識から決める以外に、データからハイパーパラメータを推定する、経験ベイズ推定がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータΦを周辺化した周辺尤度は\n",
    "\n",
    "$p(W|\\beta)=\\int p(W|\\phi)p(\\phi|\\beta)d\\phi $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これが最大になるようなβを求めればいい。不動点反復法を用いて最大になるようなβは求まる。\n",
    "\n",
    "ハイパーパラメータの事前分布を考えて、さらに事後分布を求めるようなこともできる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 混合ユニグラムモデル\n",
    "### 3.1 混合ユニグラムモデル\n",
    "ある文章dにおける単語の集合が$w_d$になる確率は\n",
    "\n",
    "$p(w_d|\\theta,\\phi)=\\sum_{k=1}^K \\theta_k \\Pi_{v=1}^V \\phi_{kv}^{N_{dv}}$\n",
    "\n",
    "文章の集合Wが得られる確率は\n",
    "\n",
    "$p(W|\\theta,\\phi)=\\Pi_d p(w_d|\\theta,\\phi)$\n",
    "\n",
    "### 3.3 EMアルゴリズム\n",
    "対数尤度は\n",
    "\n",
    "$L=\\sum^D_{d=1}\\log p(w_d|\\theta,\\Phi)=\\sum^D_{d=1}\\log\\sum p(w_d|\\phi_k)p(k|\\theta)$\n",
    "\n",
    "これを最大にするパラメータは解析的に求まらない。そこでEMアルゴリズムを使う。EMアルゴリズムは対数尤度の下限を最大化することでパラメータの局所最適値を求める。イェンゼンの不等式を用いると、対数尤度の下限Fが計算できる。\n",
    "\n",
    "イェンゼンの不等式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$L=\\sum^D_{d=1}\\log\\sum_{k=1}^K q_{dk} \\frac{p(w_d|\\phi_k)p(k|\\theta)}{q_{dk}}$  \n",
    "$\\geq \\sum^D_{d=1}\\sum_{k=1}^{K}q_{dk} \\log\\frac{p(w_d|\\phi_k)p(k|\\theta)}{q_{dk}}$  \n",
    "$=\\sum^D_{d=1}\\sum_{k=1}^{K}q_{dk} (\\log\\theta_k + \\sum^V_{v=1}N_{dv}\\log \\phi_{kv} - \\log q_{dk}) \\equiv F$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q_{dk}$は負担率。文章dがトピックkに属する確率。logの中のΣが外に出ることで、解析的に解ける。\n",
    "\n",
    "EMアルゴリズムは、EステップとMステップを収束するまで繰り返す。\n",
    "\n",
    "Eステップ：Fを最大化するqを求める。\n",
    "\n",
    "Mステップ：Fを最大化するΦとθを求める。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 変分ベイズ推定\n",
    "#### 3.4.1 周辺尤度の最大化\n",
    "混合ユニグラムモデルでは解析的に事後分布が計算できない。そこで変分ベイズ推定を使う。\n",
    "\n",
    "$\\Psi=\\{\\theta,\\Phi\\}$とする。変分ベイズ推定では、周辺尤度\n",
    "\n",
    "$p(W)=\\int \\sum_z p(W,z,\\Psi) d\\Psi$\n",
    "\n",
    "を最大化するトピックおよびパラメータの事後分布$p(z|W),p(\\Psi|W)$の近似を求める。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 ギブスサンプリング\n",
    "#### 3.5.1 MCMC\n",
    "ギブスサンプリングはMCMCの一種。\n",
    "\n",
    "例えば、分布$p(z_1,\\cdots, z_D|W)$を推定したいとする。ギブスサンプリングでは、変数$z_d$以外のすべての変数が与えられたときの$z_d$の条件付き確率$p(z_d|z_1,\\cdots,z_{d-1},z_{d+1},\\cdots, z_D,W)$に従って$z_d$の値をサンプリングすることを全ての変数に対して繰り返すことで、目的の分布からの事例をえる。十分多くの事例が得られれば、目的の分布は次の経験分布で近似できる。\n",
    "\n",
    "$p(z_1, \\cdots, z_D|W) \\simeq \\frac{1}{S} \\sum^S_{s=1} \\delta ((z_1, \\cdots, z_D),(z_1^{(s)}, \\cdots,z_D^{(s)} ))$\n",
    "\n",
    "#### 3.5.2 パラメータの周辺化\n",
    "混成ユニグラムモデルにはz,θ,Φの3つの未知変数がある。ここでは、θ、Φは周辺化し、トピック集合zの事後分布を推定する。パラメータを周辺化するギブスサンプリングは崩壊型ギブスサンプリングと呼ばれる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. トピックモデル\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. トピック数の推定\n",
    "ディリクレ過程を用いることで、混合モデルのトピック数を推定できる。ディリクレ過程は、基底分布Hと集中パラメータαによって規定される、離散分布Gを生成する確率分布\n",
    "\n",
    "$G\\sim DP(\\alpha, H)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.2 中華料理店過程\n",
    "d番目の文書のトピック$z_d$がkである確率は、d-1番目までの文書のトピックに依存し、\n",
    "\n",
    "$p(z_d=k|z_1,\\cdots,z_{d-1},\\alpha) = \\left\\{\\begin{array}{l} \\frac{D_k}{d-1+\\alpha} \\\\ \\frac{\\alpha}{d-1+\\alpha} \\end{array} \\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D_k$はトピックkが割り当てられた文書数、αは集中パラメータ。新しいトピックが選ばれる確率はαに比例する。\n",
    "\n",
    "崩壊型ギブスサンプリングを用いてトピックを推定する場合、文書d以外のトピック集合$z_{\\backslash d}=(z_1, \\cdots, z_{d-1}, z_{d+1}, \\cdots, z_D)$が与えられたときの、文章dのトピック$z_d$の条件付き確率が必要となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
