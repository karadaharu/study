{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev2\"><a href=\"#Lesson2-:-Markov-Decision-Process\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Lesson2 : Markov Decision Process</a></div><div class=\"lev2\"><a href=\"#Lesson-3-:-Temporal-difference-learning\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>Lesson 3 : Temporal difference learning</a></div><div class=\"lev3\"><a href=\"#TD(1)-learning\"><span class=\"toc-item-num\">0.2.1&nbsp;&nbsp;</span>TD(1) learning</a></div><div class=\"lev3\"><a href=\"#TD(0)\"><span class=\"toc-item-num\">0.2.2&nbsp;&nbsp;</span>TD(0)</a></div><div class=\"lev3\"><a href=\"#TD(λ)\"><span class=\"toc-item-num\">0.2.3&nbsp;&nbsp;</span>TD(λ)</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3つのleaning\n",
    "\n",
    "* supervised learning $y=f(x)$ function approx.\n",
    "* unsupervised learning : $f(x)$\n",
    "* reinforcement learning $y=f(x), z$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid world\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4665600000000002"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8*0.8*0.9*0.9*0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3277600000000001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8**5+0.1**4*0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson2 : Markov Decision Process\n",
    "* states :$s$\n",
    "* model : Model $T(s,a,s') \\sim Pr(s'|s,a)$ モデルがひとつ前の状態にしか依存しない→Markov\n",
    "* actions : $A(s), A$\n",
    "* reward : $R(s),R(s,a),R(s,a,s')$\n",
    "\n",
    "solution\n",
    "* policy $\\pi(s) \\rightarrow a$\n",
    "\n",
    "$s,a,r$のペアから$pi$をえる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "リワードの設計によって迷路の最適解は変わって学習の結果がかわる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "infinite horizons : 寿命を考えると迷路の同じ位置でも状態は変わる\n",
    "\n",
    "utilityにdiscountをいれないで、ただrewordを足すと正のリワードが得られる場合、無限の時間を考えると、どんなrewordでもutilityは無限で比較できなくなってしまう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best policies\n",
    "\n",
    "$$\n",
    "\\pi^* := \\rm{arg} \\max_\\pi E[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t) | \\pi]\n",
    "$$\n",
    "\n",
    "ある状態のutilityは（rewordとは違う)\n",
    "\n",
    "$$\n",
    "U^\\pi(s) := E[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t)|\\pi,s_0=s]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以降では$U(s):=U^{\\pi^*}(s)$とする\n",
    "\n",
    "最適戦略のもとでのつぎの状態s'のutilityの期待値を最大化するようなアクションが最適 : \n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\rm{arg}\\max_a \\sum_{s'} T(s,a,s') U(s')\n",
    "$$\n",
    "\n",
    "再帰的に書けば\n",
    "\n",
    "$$\n",
    "U(s) = R(s) + \\gamma \\max_a \\sum_{s'} T(s,a,s') U(s')\n",
    "$$\n",
    "\n",
    "これはBellman方程式と呼ばれる\n",
    "\n",
    "Bellman方程式を解きたい。状態数を$n$とすれば、方程式の数は$n$、未知変数は$U(s)$だけなので、$n$個。$\\max$によって方程式は非線形になっていて、解析的に解けない。\n",
    "\n",
    "適当な$\\hat{U}_t(s)$から初めて、\n",
    "\n",
    "$$\n",
    "\\hat{U}_{t+1}(s) = R(s) + \\gamma \\max_a \\sum_{s'} T(s,a,s') \\hat{U}_t(s')\n",
    "$$\n",
    "\n",
    "と更新する。(value iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんでいいのかというと、$R(s)$が真のrewardなので、いいっぽい。\n",
    "なんか局所解にはまりそうだけどなぁ。もっといい$\\max$を解く数値計算方法？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理解したいこと:利得がgivenじゃないとき？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迷路の計算例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37600000000000006"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_1_x = -0.04 + 0.5 * (0.8*1+0.2*0)\n",
    "u_1_xd = -0.04\n",
    "u_2_x = -0.04 + 0.5 *(0.8*1+0.1*u_1_x+0.1*u_1_xd)\n",
    "u_2_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "別に完璧なUを求める必要はない。policy $\\pi$さえわかればいい→policy iteration\n",
    "\n",
    "まず適当なpolicy $\\pi_0$を決める\n",
    "\n",
    "(1) $\\pi_t$があったときに$U_t=U^{\\pi_t}$を計算する\n",
    "\n",
    "$$\n",
    "U_t(s)= -R(s) + \\gamma \\sum_{s'} T(s,\\pi_t(s), s')U_t(s')\n",
    "$$\n",
    "\n",
    "(2) 更新する $\\pi_{t+1}=\\rm{arg}\\max_a \\sum T(s,a,s') U(s')$\n",
    "\n",
    "ポイントは$U$の更新に$\\max$がなくなったので、線形になった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman Equation(より一般に)\n",
    "\n",
    "$$\n",
    "V(s) = \\max_a(R(s,a)+\\gamma \\sum_{s'} T(s,a,s') V(s'))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さっきの$R(s)$を$R(s,a)$にして、$U$を$V$にした。$V$をvalue functionと呼ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V(s) = \\max_a (R(s,a)+\\gamma\\sum_{s'} T(s,a,s') (\\max_a(R(s,a)+\\gamma\\sum_{s'} T(s,a,s')\\cdots]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なので、この再帰を違うふうに区切ることもできる\n",
    "\n",
    "$$\n",
    "Q(s,a) = R(s,a) + \\gamma \\sum_{s'} T(s,a,s') \\max_a Q(s,a)\n",
    "$$\n",
    "\n",
    "Quality functionと呼ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もうひとつ考えようとおもうと、\n",
    "\n",
    "$$\n",
    "C(s,a) = \\gamma \\sum_{s'} T(s,a,s')\\max_{a'} (R(s',a')+C(s',a'))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これをContinuation functionとよぶ。すべてをBellman equationとよぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V(s) = \\max_a(Q(s,a)) = \\max_a(R(s,a)+C(s,a))\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(s,a) = R(s,a) +\\gamma\\sum_{s'} T(s,a,s')V(s') = R(s,a) + C(s,a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "C(s,a) = \\gamma \\sum_{s'} T(s,a,s')V(s') = \\gamma \\sum_{s'} (T(s,a,s')\\max_{a'}Q(s',a'))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cはその状態からそのアクションをとったときの未来のValueの期待値\n",
    "\n",
    "Vはその状態で一番いいアクションをとったときの..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学的には$R(s,a)$は$R(s)$と別の状態を考えれば書けるんだけど、この差が重要になってくる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 : Temporal difference learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "環境が分からないのがreinforcement learningとMDPの違い\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* plan : fixed sequence of actions\n",
    "* conditional plan : include \"if\" but fixed\n",
    "* stationary policy : mapping from state to action→large\n",
    "\n",
    "policyの評価:そのpolicyでうまれるutilityの期待値\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6400000000000001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8**2 + 0.8**4 + 0.8**2 + (-0.2)*0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(1) learning\n",
    "エピソード$T$に対して、すべての状態$s$のeligibility$e(s)=0$、value関数$V_T(s)=V_{T-1}(s)$で初期化する。\n",
    "\n",
    "1ステップ$s_{t-1}\\rightarrow^{r_t}s_t$動いたら  \n",
    "eligibilityを更新 $e(s_{t-1})\\leftarrow e(s_{t-1})+1$  \n",
    "すべての状態$s$について  \n",
    "\n",
    "Value関数の更新:\n",
    "$$V_T(s) \\leftarrow V_T(s) + \\alpha_T (r_t + \\gamma V_{T-1}(s_t)-V_{T-1}(s_{t-1}))e(s)$$\n",
    "eligibilityの更新:\n",
    "$e(s) \\leftarrow \\gamma e(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例として$s_1\\rightarrow^{r_2} s_2 \\rightarrow^{r_3} s_3 \\rightarrow^{r_4} s_4\\cdots  \\rightarrow^{r_n} s_n $を考える。各ステップでの$s_1$についてのvalue関数は\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta V_T(s_1)(1) &= \\alpha_T (r_2 + \\gamma V_{T-1}(s_2) - V_{T-1} (s_1))  \\gamma^0 \\\\\n",
    "\\Delta V_T(s_1)(2) &= \\alpha_T (r_3 + \\gamma V_{T-1}(s_3) - V_{T-1} (s_2))  \\gamma^1 \\\\\n",
    "\\Delta V_T(s_1)(3) &= \\alpha_T (r_4 + \\gamma V_{T-1}(s_4) - V_{T-1} (s_3)) \\gamma^2 \\\\\n",
    "&\\vdots \\\\\n",
    "\\Delta V_T(s_1)(n-1) &= \\alpha_T (r_n + \\gamma V_{T-1}(s_n) - V_{T-1} (s_{n-1})) \\gamma^{n-2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "全ステップを足すと、\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta V_T(s_1) = \\alpha_T(\\sum_{i=0} r_{i+2} \\gamma^i +\\gamma^{n-1} V_{T-1}(s_n) - V_{T-1}(s_1))\n",
    "$$\n",
    "\n",
    "となって、未来の状態のvalue関数を割り引きながら足したものとの差分を更新することになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんですべての状態のvalue関数についてある状態にうつったときのvalue関数を足すの？\n",
    "→そのエピソードにおいてその状態にうつらなければ、$e(s)=0$だから、足されない。1ステップごとに$e(s)$に$\\gamma$で割り引かれるので、未来のrewardも伝搬するようになっている。\n",
    "\n",
    "TD(1) learningの問題点 : 更新に一回のエピソードしか使わないこと。全観測エピソードのなかで一回しか出てこなかった状態についてのバイアスがすごい大きいままになる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) \n",
    "1ステップ動いたら、前にいた状態$s_{t-1}$についてだけ: \n",
    "\n",
    "$$\n",
    "V_T(s_{t-1}) \\leftarrow V_T(s_{t-1}) + \\alpha_T (r_t + \\gamma V_{T-1}(s_t)-V_{T-1}(s_{t-1}))e(s) \n",
    "$$\n",
    "\n",
    "こうすれば、ある状態についての更新が、偶然出た経路によって変にバイアスされない。これは最尤推定していることになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(λ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エピソード$T$に対して、すべての状態$s$のeligibility$e(s)=0$、value関数$V_T(s)=V_{T-1}(s)$で初期化する。\n",
    "\n",
    "1ステップ$s_{t-1}\\rightarrow^{r_t}s_t$動いたら  \n",
    "eligibilityを更新 $e(s_{t-1})\\leftarrow e(s_{t-1})+1$  \n",
    "すべての状態$s$について  \n",
    "\n",
    "Value関数の更新:\n",
    "$$V_T(s) \\leftarrow V_T(s) + \\alpha_T (r_t + \\gamma V_{T-1}(s_t)-V_{T-1}(s_{t-1}))e(s)$$\n",
    "eligibilityの更新:\n",
    "$e(s) \\leftarrow \\lambda\\gamma e(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eligibilityの更新を$\\lambda$割り引くだけ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD(λ)は何をしているか？\n",
    "k-step estimaterを次にように考える\n",
    "\n",
    "あるエピソード$T$での更新として\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "1& : V_T(s_t) \\leftarrow  V_T(s_t)  + \\alpha_T (r_{t+1} + \\gamma V_{T-1} (s_{t+1})  - V_{T-1}(s_t)) \\\\\n",
    "2 &: V_T(s_t) \\leftarrow  V_T(s_t)  + \\alpha_T (r_{t+1} + \\gamma V_{T-1} (s_{t+1}) + \\gamma^2 V_{T-1} (s_{t+2}) - V_{T-1}(s_t)) \\\\\n",
    "& \\vdots \\\\\n",
    "\\infty&  : V_T(s_t) \\leftarrow  V_T(s_t)  + \\alpha_T (r_{t+1} + \\gamma V_{T-1} (s_{t+1}) + \\gamma^2 V_{T-1} (s_{t+2})  + \\cdots - V_{T-1}(s_t)) \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1　step がTD(0)、$\\infty$ステップがTD(1)に対応している。それぞれのestimaterに対して\n",
    "$$\n",
    "\\begin{aligned}\n",
    "1& :(1-\\lambda)\\\\\n",
    "2 &: \\lambda(1-\\lambda)\\\\\n",
    "& \\vdots \\\\\n",
    "\\infty& : \\lambda^\\infty\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "を重みづけして考えていることになるらしい。たしかに、どんどん$\\lambda$で落ちていくのでそうなってそう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
