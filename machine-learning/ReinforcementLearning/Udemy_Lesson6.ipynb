{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reward関数を変える必要があるのはなぜか？\n",
    "\n",
    "真のrewardは知らないから\n",
    "なるべく簡単に解くため\n",
    "\n",
    "最適なpolicyを変えずにreward関数を変えるには？\n",
    "\n",
    "rewardに正の実数をかけたり、実数を変えてもよさそう。その他非線形な方法(potential based)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(s,a) = R(s,a) + \\gamma \\sum_{s'} T(s,a,s') \\max_{a'} Q(s',a')\n",
    "$$\n",
    "\n",
    "\n",
    "$R'(s,a) = cR(s,a)$のとき、\n",
    "\n",
    "$Q'=cQ$とすれば、\n",
    "\n",
    "$$\n",
    "cQ(s,a) = cR(s,a) + \\gamma \\sum_{s'} T(s,a,s') \\max_{a'} cQ(s',a')\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q'(s,a) = R'(s,a) + \\gamma \\sum_{s'} T(s,a,s') \\max_{a'} Q'(s',a')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q'=Q+\\alpha$のとき\n",
    "\n",
    "$$\n",
    "Q(s,a)+\\alpha = R'(s,a) + \\gamma \\sum_{s'} T(s,a,s') \\max_{a'} (Q(s',a') +\\alpha)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(s,a)  = R(s,a) -\\alpha  +  \\gamma \\alpha + \\gamma \\sum_{s'} T(s,a,s') \\max_{a'} Q(s',a') \n",
    "$$\n",
    "\n",
    "だから、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$R'(s,a)=R(s,a)+C$のとき、\n",
    "\n",
    "$Q'=Q+C / (1-\\gamma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reward shaping\n",
    "\n",
    "どうやってrewardを設計するか？\n",
    "\n",
    "サッカーロボット：\n",
    "\n",
    "ボールへの近さでreward、ボールを蹴るでreward、ボールのゴールへの近さでreward\n",
    "\n",
    "potential based shaping\n",
    "\n",
    "状態の変化にrewardを設定すれば、振動したりしなくなる\n",
    "\n",
    "$$\n",
    "Q(s,a)  = \\gamma \\sum_{s'} T(s,a,s') (R(s,a,s')   +\\gamma\\max_{a'} Q(s',a') )\n",
    "$$\n",
    "\n",
    "$R'(s,a,s') = R(s,a) -\\Psi(s) + \\gamma \\Psi(s')$とする。意味は、s'に移動したら、sにいるrewardは失う\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このときの$Q'$は\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q'& = \\sum_{s'} T(R'+\\gamma \\max Q')\\\\\n",
    "Q'&= R -\\Psi +\\gamma \\sum T \\max(Q'+\\Psi)\n",
    "Q'+\\Psi&= R +\\gamma \\sum T \\max(Q'+\\Psi)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "だから$Q'=Q-\\Psi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q learningで初期値をポテンシャルと同じ形であたえれば同じことになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
