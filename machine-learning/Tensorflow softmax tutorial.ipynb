{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev2\"><a href=\"#Sotfmax-Regressions\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Sotfmax Regressions</a></div><div class=\"lev2\"><a href=\"#implementing-the-regression\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>implementing the regression</a></div><div class=\"lev2\"><a href=\"#Training\"><span class=\"toc-item-num\">0.3&nbsp;&nbsp;</span>Training</a></div><div class=\"lev2\"><a href=\"#evaluating-our-model\"><span class=\"toc-item-num\">0.4&nbsp;&nbsp;</span>evaluating our model</a></div><div class=\"lev2\"><a href=\"#backpropagation\"><span class=\"toc-item-num\">0.5&nbsp;&nbsp;</span>backpropagation</a></div><div class=\"lev3\"><a href=\"#computational-graphs\"><span class=\"toc-item-num\">0.5.1&nbsp;&nbsp;</span>computational graphs</a></div><div class=\"lev3\"><a href=\"#derivatives-on-computational-graphs\"><span class=\"toc-item-num\">0.5.2&nbsp;&nbsp;</span>derivatives on computational graphs</a></div><div class=\"lev3\"><a href=\"#factoring-paths\"><span class=\"toc-item-num\">0.5.3&nbsp;&nbsp;</span>factoring paths</a></div><div class=\"lev2\"><a href=\"#分からないこと\"><span class=\"toc-item-num\">0.6&nbsp;&nbsp;</span>分からないこと</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST(mixed national institude standards and technology) dataset : 手書きの数字データセット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TensorFlowEstimator : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot vectors : ある次元だけ1で他は0のベクトル（0-9の範囲で3なら3次元めだけ1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sotfmax Regressions\n",
    "入力x、あるクラスiである証拠は\n",
    "\n",
    "$\\rm{evidence}_{i} = \\sum_{j} W_{ij}x_{j} + b_{i} $\n",
    "\n",
    "証拠から確率に変換するソフトマックスは正規化された指数関数で表される\n",
    "\n",
    "$y=\\rm{softmax}(\\rm{evidence})$\n",
    "\n",
    "$\\rm{softmax}(x)_{i}=\\frac{\\exp(x_{i}}{\\sum_{j}\\exp(x_{j})}$\n",
    "\n",
    "これはポテンシャルエネルギーと確率の関係に対応していそう。\n",
    "\n",
    "$y=\\rm{softmax}(Wx+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementing the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xは\"placeholder\"と呼ばれ、計算を実行するときに値が入る。Noneはどんな長さでもいいことを表す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variablesは計算中に値を変えることができるもので、パラメーターはVariableで表す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmaxを計算するには、単にベクトルをsoftmax関数に渡すだけ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "学習するにはコストを定義する必要がある。よく使われるのは、\"Cross-entropy\"\n",
    "\n",
    "$H_{y'}(y)=-\\sum_{i} y'_{i} \\log (y_{i})$\n",
    "\n",
    "y: 予測した確率分布  \n",
    "y': 真の分布\n",
    "\n",
    "真の分布は学習データから求めるの？  \n",
    "なんで分布を一致させるだけでいい感じに予測できるの？\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さっきはWが行列だったので、matmulが必要だったけど、yはベクトルなので\\*でOK。yはテンソルで1次元目はデータのインデックスなので2次元目について平均を計算する。\n",
    "\n",
    "\"backpropagation algorithm\"を使って自動的にコストが小さくなるように計算される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x : batch_xs, y_:batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluating our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9179\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "色々な分野で使われいて、応用と関係ない一般的な名前は\"reverse-mode differentiation\"\n",
    "\n",
    "微分を速く計算するテクニック。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computational graphs\n",
    "$e = (a+b) (b+1)$を\n",
    "\n",
    "$c = a+b \\\\\n",
    "d = b+1\\\\\n",
    "e = c d$\n",
    "\n",
    "としてグラフで表す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### derivatives on computational graphs\n",
    "cがaの関数のとき、aを少しかえたとき、cはどうなるのか？これがcのaについての偏微分。\n",
    "\n",
    "a = 2, b=1のとき、\n",
    "\n",
    "$\\frac{\\partial c}{\\partial a} = 1\\\\\n",
    "\\frac{\\partial c}{\\partial b} = 1\\\\\n",
    "\\frac{\\partial d}{\\partial b} = 1\\\\\n",
    "\\frac{\\partial e}{\\partial c} = 2\\\\\n",
    "\\frac{\\partial e}{\\partial d} = 3\\\\\n",
    "$\n",
    "\n",
    "と計算できる。\n",
    "\n",
    "eのa,bについての偏微分も足し合わせればよい。\n",
    "\n",
    "aが速度1でかわるとcも速度1でかわる。それはeが速度2でかわることになる。なので、 eはaに対して1\\*2でかわる。\n",
    "\n",
    "bについては、bが1でかわるとc、dがそれぞれ1でかわる。c,dがそれぞれ1でかわるとeはそれぞれ2,3でかわるので、すべてたしあわせて、1\\*2+1\\*3でかわる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### factoring paths\n",
    "パスが複数あると、単純に足し合わせると複雑なグラフになると足し算の項が指数的に増えてしまうという問題がある。->各ノードでかけることをにする\n",
    "\n",
    "$\\frac{\\partial Z}{\\partial X} = (\\alpha + \\beta + \\gamma) (\\delta + \\epsilon + \\xi)$\n",
    "\n",
    "これが\"forward-mode differentiation\"と\"reverse-mode differentiation\"のもとになっているもの。\n",
    "\n",
    "この計算のしかたに従って、ルートノードから順に$\\frac{\\partial Z}{\\partial}$をかけていくのがバックプロパゲーション\n",
    "\n",
    "末端ノードから計算するとすべての末端ノードからの計算をする必要があるが、ルートから計算すると一度にできる。\n",
    "\n",
    "ニューラルネットワークにおいて、どのパラメーターをかえるとコストが小さくなるのかを見つけるために、微分を計算する必要があるので、このときに使われる。Gradient descentなどで最小値を探す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 分からないこと\n",
    "具体的にどうやってパラメーターを更新しているのか？（バックプロパゲーションで偏微分を計算して探すっぽいことは分かった）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
