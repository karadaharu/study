{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev2\"><a href=\"#最小二乗法\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>最小二乗法</a></div><div class=\"lev2\"><a href=\"#非線形最小二乗法\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>非線形最小二乗法</a></div><div class=\"lev3\"><a href=\"#Gauss-Newton法\"><span class=\"toc-item-num\">0.2.1&nbsp;&nbsp;</span>Gauss-Newton法</a></div><div class=\"lev3\"><a href=\"#Levenberg-Marquardt法\"><span class=\"toc-item-num\">0.2.2&nbsp;&nbsp;</span>Levenberg-Marquardt法</a></div><div class=\"lev3\"><a href=\"#Trust-Region-Reflective-Algorithm\"><span class=\"toc-item-num\">0.2.3&nbsp;&nbsp;</span>Trust Region Reflective Algorithm</a></div><div class=\"lev2\"><a href=\"#References\"><span class=\"toc-item-num\">0.3&nbsp;&nbsp;</span>References</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最小二乗法\n",
    "測定データyは\n",
    "\n",
    "$y=f(x)+\\epsilon$\n",
    "\n",
    "f(x)は既知の関数g(x)の線形結合で表されているとする\n",
    "\n",
    "$f(x, a) = \\sum_{k} a_{k}g_{k}(x)$\n",
    "\n",
    "誤差の分散の推定値は、残差の平方和\n",
    "\n",
    "$J=\\sum_{i}(y_{i} - f(x_{i}, a))^{2}$\n",
    "\n",
    "で与えられるから、Jが最小になるaを求めればいい。\n",
    "\n",
    "関数fは\n",
    "\n",
    "$f(x) = (g_{1}(x), g_{2}(x) , \\cdots , g_{m}(x)) (a_{1}, a_{2}, \\cdots, a_{m})^{T}$\n",
    "\n",
    "と表せる。目的関数Jは\n",
    "\n",
    "\n",
    "## 非線形最小二乗法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のアルゴリズムはすべて局所最適化するためのアルゴリズム。目的関数が複数の局所最適解をもっているときには初期値に依存してしまう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的関数は\n",
    "\n",
    "$f(x) = \\frac{1}{2} \\sum^{m}_{j=1}r_{i}^{2}(x)$\n",
    "\n",
    "ここでr(x)は残差関数$\\mathbb{R}^{N}\\rightarrow \\mathbb{R}$\n",
    "\n",
    "ヤコビアンは\n",
    "\n",
    "$J_{ij}(x)=\\frac{\\partial r_{i}(x)}{\\partial x_{j}}$\n",
    "\n",
    "領域に制約がないとき最小値である必要条件は、極値であることです。\n",
    "\n",
    "したがって、gradientのベクトルが0になるような点を見つける問題に帰着されます。線形の場合には、連立方程式になって行列の計算で解析解を得ることができましたが、非線形の場合には反復法で解くことになります。\n",
    "\n",
    "グラディエント\n",
    "\n",
    "$\\nabla f(x) = [\\frac{\\partial}{\\partial x_{j}}] f(x) \\\\\n",
    "=[\\sum_{i} r_{i} \\frac{\\partial r_{j}}{\\partial x_{j}}]= J^{T}r(x)$\n",
    "\n",
    "$\\nabla f(x) = 0$\n",
    "\n",
    "を見つけるシンプルな方法としてニュートン法があります。\n",
    "\n",
    "### Gauss-Newton法\n",
    "ニュートン法においては、$\\nabla f(x) = g(x)$としたとき、\n",
    "\n",
    "ステップ$\\delta x$は\n",
    "\n",
    "$\\delta x = - J_{g}^{-1}g(x)$\n",
    "\n",
    "gについてのヤコビアン$J_{g}$は\n",
    "\n",
    "$J_{g} = \\nabla^{2} f$\n",
    "\n",
    "なので、\n",
    "\n",
    "$\\nabla^{2} f \\delta x = \\nabla f(x)$\n",
    "\n",
    "となります。ここで、ヘシアン$\\nabla^{2} f$を\n",
    "\n",
    "$\\nabla^{2} f(x) = J^{T}J$と近似するのがGauss-Newton法です。\n",
    "\n",
    "ヘシアン:二階導関数の正方行列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenberg-Marquardt法\n",
    "Levenberg-Marquardt法の基本的なアイデアは、Gauss-Newtonステップに対して正則化項を加えたものを使うというものです。\n",
    "\n",
    "$J^{T}_{k}J_{k}+\\alpha_{k}\\delta x_{k}^{LM}=-J^{T}_{k}f_{k}$\n",
    "\n",
    "$\\alpha_{k}$は前回のステップがどれくらい良かったかによって決まる指標になっています。\n",
    "$\\alpha_{k}$が小さいときは、Gauss-Newtonステップになり、$\\alpha_{k}$が大きいときは、勾配と逆のほうにステップをとることで大域的に収束するようにします。\n",
    "\n",
    "Lvevenberg-Marquadt法は信頼領域のアルゴリズムとして見ることができます。\n",
    "\n",
    "信頼領域のアプローチでは各イテレーションのステップサイズを次の制約付き2次部分問題を解くことで得ます。ステップpは\n",
    "\n",
    "$\\min_{p} m_{k}(p)=\\frac{1}{2}p^{T}B_{k}p+g_{k}^{T}p, \\: s.t. ||p||\\leq \\Delta_{k}$\n",
    "\n",
    "ここでの信頼領域$\\Delta_{k}$と正則化項$\\alpha_{k}$が対応していることが次のことから分かります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust Region Reflective Algorithm\n",
    "scipyで実装されているもの。\n",
    "\n",
    "まず先ほど$J^{T}T$で近似したヘシアンが既知の問題について考えます。\n",
    "\n",
    "最小化問題は一般に次のように書けます。\n",
    "\n",
    "$\\min f(x),\\: x \\in \\mathcal{F} = \\{x : l \\leq x \\leq u \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "なんで微分できない点があるのにヤコビアンがあるの？\n",
    "\n",
    "結局大域最適ではない？\n",
    "\n",
    "いま考えている問題はどうなの？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "* [非線形最小二乗法](https://ja.wikipedia.org/wiki/%E9%9D%9E%E7%B7%9A%E5%BD%A2%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%97%E6%B3%95)\n",
    "* [Basic Algorithms for Nonlinear Least Squares](https://nmayorov.wordpress.com/2015/06/18/basic-algorithms-for-nonlinear-least-squares/)\n",
    "* [Trust Region Reflective Algorithm](https://nmayorov.wordpress.com/2015/06/19/trust-region-reflective-algorithm/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
